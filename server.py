from __future__ import annotations

import ast
import io
import json as _json
import os, sys
import re
import shutil
import sys
import tempfile
import urllib.request
import zipfile
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from dotenv import load_dotenv
from fastapi import FastAPI, File, Form, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.responses import PlainTextResponse
from fastapi.staticfiles import StaticFiles


# autoparts.py aynı klasörde olmalı
from autoparts import (
    _comp_lines as comp_lines,
    ai_chat_ollama,
    ai_suggest_name,
    build_components,
    extract_main_block_code,
    extract_top_level_items,
    render_init,
    render_main,
    render_module,
    repack_components,
)

load_dotenv()

APP_NAME = "autoparts API (Ollama, non-empty-safe)"
DEFAULT_OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
DEFAULT_OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gpt-oss:20b")

# -------------------------------- App --------------------------------

app = FastAPI(title=APP_NAME)

# CORS: geliştirme için geniş açık. İstersen belirli origin'lere daralt.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------------------------- Routes: Health --------------------------


@app.get("/")
def root():
    return {
        "ok": True,
        "name": APP_NAME,
        "endpoints": [
            "POST /plan",
            "POST /plan_multi",
            "POST /build",
            "POST /build_multi",
            "POST /ai_edit",
            "POST /ai_edit_multi",
            "GET  /health",
        ],
    }


@app.get("/health")
def health():
    return {"status": "up"}


# ----------------------------- Utilities -----------------------------


def resource_path(rel):
    base = getattr(sys, "_MEIPASS", None)  # PyInstaller içinden çalışırken
    return os.path.join(base if base else os.path.abspath("."), rel)


STATIC_DIR = resource_path("ui/dist")
if os.path.isdir(STATIC_DIR):
    app.mount("/", StaticFiles(directory=STATIC_DIR, html=True), name="static")


def _clamp(v: int, lo: int, hi: int) -> int:
    return max(lo, min(hi, v))


def apply_compact(
    level: int, psl: int, mm: int, mx: int, tgt: Optional[int]
) -> Tuple[int, int, int, Optional[int]]:
    """
    level: 0..3
      - psl: pack_small_lines
      - mm: min_module_lines
      - mx: max_modules
    """
    level = _clamp(level or 0, 0, 3)
    if level == 0:
        return psl, mx, mm, tgt
    if level == 1:
        psl = max(psl, 80)
        mx = min(mx, 10)
        mm = max(mm, 40)
    elif level == 2:
        psl = max(psl, 120)
        mx = min(mx, 8)
        mm = max(mm, 80)
        tgt = 8 if tgt is None else min(tgt, 8)
    elif level == 3:
        psl = max(psl, 160)
        mx = min(mx, 6)
        mm = max(mm, 120)
        tgt = 6 if tgt is None else min(tgt, 6)
    return psl, mx, mm, tgt


def analyze_source(
    src: str,
    *,
    pack_small_lines: int,
    max_modules: int,
    min_module_lines: int,
    target_modules: Optional[int],
) -> Dict[str, Any]:
    tree = ast.parse(src)
    items, imports, main_block, mod_doc = extract_top_level_items(src, tree)
    components, name_to_module = build_components(items)
    components, name_to_module = repack_components(
        base_file_stem="input",
        components=components,
        name_to_module=name_to_module,
        group_assignments=True,
        pack_small_lines=pack_small_lines,
        max_modules=max_modules,
        min_module_lines=min_module_lines,
        target_modules=target_modules,
    )
    comp_list = [
        {"module_name": c.module_name, "names": c.names, "lines": comp_lines(c)}
        for c in components
    ]
    # Bağımlılık kenarları
    edges = []
    for c in components:
        own = set(c.names)
        deps_mods = set()
        for it in c.items:
            for d in it.deps:
                if d not in own:
                    mod = name_to_module.get(d)
                    if mod and mod != c.module_name:
                        deps_mods.add(mod)
        for m in sorted(deps_mods):
            edges.append({"from": c.module_name, "to": m})
    return {
        "components": comp_list,
        "imports": imports,
        "has_main": main_block is not None,
        "module_doc": (ast.get_docstring(tree) or "").strip(),
        "edges": edges,
    }


def _inject_future_annotations(py_text: str) -> str:
    lines = py_text.splitlines()
    if any(l.strip() == "from __future__ import annotations" for l in lines[:5]):
        return py_text
    insert_at = 0
    for i, l in enumerate(lines[:10]):
        if l.startswith("# Generated by autoparts.py"):
            insert_at = i + 1
            break
    lines.insert(insert_at, "from __future__ import annotations")
    return "\n".join(lines)


def _postprocess_module_file(path: Path) -> None:
    txt = path.read_text(encoding="utf-8")
    txt2 = _inject_future_annotations(txt)
    if txt2 != txt:
        path.write_text(txt2, encoding="utf-8")


def _write_runner_single(
    pkg_name: str, out_dir: Path, modules: List[str], has_main: bool
) -> None:
    runner = f"""# Auto-generated runner for package: {pkg_name}
import importlib, inspect, runpy, sys
PKG = "{pkg_name}"; MODULES = {modules!r}; HAS_MAIN = {bool(has_main)}
def _call_if_exists(mod, names=('main','run','demo')):
    for nm in names:
        fn = getattr(mod, nm, None)
        if callable(fn):
            try:
                sig = inspect.signature(fn)
                if all(p.default is not inspect._empty or p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD) for p in sig.parameters.values()): fn()
                elif len(sig.parameters)==0: fn()
            except Exception as e: print(f"[skip] {{mod.__name__}}.{{nm}}: {{e}}")
def run_all():
    if HAS_MAIN:
        try: runpy.run_module(f"{pkg_name}.__main__", run_name="__main__")
        except ModuleNotFoundError: pass
    for m in MODULES:
        try:
            mod = importlib.import_module(f"{pkg_name}."+m); _call_if_exists(mod)
        except Exception as e: print(f"[skip] {{PKG}}.{{m}}: {{e}}")
if __name__ == "__main__":
    sel = sys.argv[1:]
    if sel:
        for m in sel:
            mod = importlib.import_module(f"{pkg_name}."+m); _call_if_exists(mod)
    else: run_all()
"""
    (out_dir / "run_all.py").write_text(runner, encoding="utf-8")


def _write_runner_multi(pkg_infos: List[Dict[str, Any]], root_out: Path) -> None:
    pkgs = [(p["name"], p["modules"], p["has_main"]) for p in pkg_infos]
    runner = f"""# Auto-generated multi-package runner
import importlib, inspect, runpy, sys
PKGS = {pkgs!r}
def _call_if_exists(mod, names=('main','run','demo')):
    for nm in names:
        fn = getattr(mod, nm, None)
        if callable(fn):
            try:
                sig = inspect.signature(fn)
                if all(p.default is not inspect._empty or p.kind in (p.VAR_POSITIONAL, p.VAR_KEYWORD) for p in sig.parameters.values()): fn()
                elif len(sig.parameters)==0: fn()
            except Exception as e: print(f"[skip] {{mod.__name__}}.{{nm}}: {{e}}")
def run_pkg(pkg_name, modules, has_main):
    if has_main:
        try: runpy.run_module(f"{{pkg_name}}.__main__", run_name="__main__")
        except ModuleNotFoundError: pass
    for m in modules:
        try:
            mod = importlib.import_module(f"{{pkg_name}}."+m); _call_if_exists(mod)
        except Exception as e: print(f"[skip] {{pkg_name}}.{{m}}: {{e}}")
def run_all():
    for name, modules, has_main in PKGS: run_pkg(name, modules, has_main)
if __name__ == "__main__":
    sel = sys.argv[1:]
    if sel:
        want = set(sel)
        for name, modules, has_main in PKGS:
            if name in want: run_pkg(name, modules, has_main)
    else: run_all()
"""
    (root_out / "run_all.py").write_text(runner, encoding="utf-8")


# ---------------- Lang directive ----------------

_LANG_DIRECTIVES = {
    "auto": "",
    "tr": "YORUMLAR ve DOCSTRING'LER TÜRKÇE olmalıdır.",
    "en": "All code comments and docstrings MUST be in English.",
    "de": "Alle Code-Kommentare und Docstrings MÜSSEN auf Deutsch sein.",
    "fr": "Tous les commentaires et docstrings DOIVENT être en français.",
    "es": "Todos los comentarios y docstrings del code DEBEN estar en español.",
    "ru": "Все комментарии и docstring ДОЛЖНЫ быть на русском.",
    "ar": "يجب أن تكون جميع التعليقات وملفات التوثيق داخل الشيفرة باللغة العربية.",
}


def _lang_directive(language: str) -> str:
    key = (language or "auto").lower()
    return _LANG_DIRECTIVES.get(key, _LANG_DIRECTIVES["auto"])


def _detect_language_label(language: str) -> str:
    lang = (language or "auto").lower()
    if lang.startswith("tr"):
        return "tr"
    if lang.startswith("en"):
        return "en"
    return "tr"  # UI varsayılanı


# ------------------- Model I/O: robustness helpers -------------------


def _strip_code_fences(text: str) -> str:
    """
    ```language
    ...code...
    ```
    ile benzeri fence'leri kaldır; içeriği koru.
    """
    if not text:
        return text
    # Çok satırlı blokları al
    text = re.sub(
        r"```(?:[a-zA-Z0-9_+\-\.]+)?\s*\n([\s\S]*?)\n```",
        r"\1",
        text,
        flags=re.MULTILINE,
    )
    # Serseri backtick'ler
    text = re.sub(r"```(?:[a-zA-Z0-9_+\-\.]+)?", "", text)
    text = text.replace("```", "")
    return text.strip()


def _quality_too_low(edited: str, original: str) -> bool:
    if not edited or not edited.strip():
        return True
    # Çok kısa veya orijinalin %20'sinden kısa ise
    if len(edited.strip()) < max(20, int(len(original) * 0.2)):
        return True
    # Orijinal ile neredeyse aynı (<= %2 fark)
    if abs(len(edited) - len(original)) <= max(20, int(len(original) * 0.02)):
        if edited.strip() == original.strip():
            return True
    # Kod belirtisi yok gibi (çok nadir ama koruyucu)
    if not re.search(r"\b(def |class |import |from )", edited):
        return True
    return False


def _ollama_generate(prompt: str, base_url: str, model: str, timeout: int = 90) -> str:
    url = f"{base_url.rstrip('/')}/api/generate"
    payload = {"model": model, "prompt": prompt, "stream": False}
    req = urllib.request.Request(url, method="POST")
    req.add_header("Content-Type", "application/json")
    try:
        with urllib.request.urlopen(
            req, data=_json.dumps(payload).encode("utf-8"), timeout=timeout
        ) as resp:
            data = _json.loads(resp.read().decode("utf-8"))
            return (data.get("response") or "").strip()
    except Exception:
        return ""


# ------------- Local deterministic fallback: basic refactor -----------


def _format_docstring_for_function(node: ast.FunctionDef, lang: str) -> str:
    """Return a minimal, placeholder-free docstring for a function."""
    name = getattr(node, "name", "function")
    args = [
        getattr(a, "arg", "")
        for a in getattr(getattr(node, "args", None), "args", [])
        if getattr(a, "arg", "")
    ]
    if lang == "en":
        return (
            f"Function {name}."
            if not args
            else f"Function {name}. Parameters: {', '.join(args)}."
        )
    # Turkish
    return (
        f"{name} fonksiyonu."
        if not args
        else f"{name} fonksiyonu. Parametreler: {', '.join(args)}."
    )


def _format_docstring_for_class(node: ast.ClassDef, lang: str) -> str:
    """Return a minimal, placeholder-free docstring for a class."""
    name = getattr(node, "name", "Class")
    return f"Class {name}." if lang == "en" else f"{name} sınıfı."


def _format_docstring_for_module(lang: str) -> str:
    """Return a minimal, placeholder-free module docstring."""
    return "Module description." if lang == "en" else "Modül açıklaması."


class _UsedNames(ast.NodeVisitor):
    def __init__(self):
        self.used = set()

    def visit_Name(self, node: ast.Name):
        if isinstance(node.ctx, ast.Load):
            self.used.add(node.id)

    def visit_Attribute(self, node: ast.Attribute):
        # np.array -> 'np'
        if isinstance(node.value, ast.Name):
            self.used.add(node.value.id)
        self.generic_visit(node)


def _remove_unused_imports(tree: ast.Module) -> ast.Module:
    used = _UsedNames()
    used.visit(tree)
    new_body = []
    for node in tree.body:
        if isinstance(node, ast.Import):
            kept = [
                a for a in node.names if (a.asname or a.name.split(".")[0]) in used.used
            ]
            if kept:
                node.names = kept
                new_body.append(node)
        elif isinstance(node, ast.ImportFrom):
            kept = [a for a in node.names if (a.asname or a.name) in used.used]
            if kept:
                node.names = kept
                new_body.append(node)
        else:
            new_body.append(node)
    tree.body = new_body
    return tree


def _ensure_docstrings(tree: ast.Module, language: str) -> ast.Module:
    lang = _detect_language_label(language)
    # module docstring
    if not (
        tree.body
        and isinstance(tree.body[0], ast.Expr)
        and isinstance(getattr(tree.body[0], "value", None), ast.Constant)
        and isinstance(tree.body[0].value.value, str)
    ):
        tree.body.insert(
            0, ast.Expr(value=ast.Constant(value=_format_docstring_for_module(lang)))
        )
    # function & class docstrings
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            if not (
                node.body
                and isinstance(node.body[0], ast.Expr)
                and isinstance(getattr(node.body[0], "value", None), ast.Constant)
                and isinstance(node.body[0].value.value, str)
            ):
                node.body.insert(
                    0,
                    ast.Expr(
                        value=ast.Constant(
                            value=_format_docstring_for_function(node, lang)
                        )
                    ),
                )
        elif isinstance(node, ast.ClassDef):
            if not (
                node.body
                and isinstance(node.body[0], ast.Expr)
                and isinstance(getattr(node.body[0], "value", None), ast.Constant)
                and isinstance(node.body[0].value.value, str)
            ):
                node.body.insert(
                    0,
                    ast.Expr(
                        value=ast.Constant(
                            value=_format_docstring_for_class(node, lang)
                        )
                    ),
                )
    return tree


def _sort_imports(tree: ast.Module) -> ast.Module:
    imports, from_imports, others = [], [], []
    for node in tree.body:
        if isinstance(node, ast.Import):
            imports.append(node)
        elif isinstance(node, ast.ImportFrom):
            from_imports.append(node)
        else:
            others.append(node)
    imports.sort(key=lambda n: ", ".join(a.asname or a.name for a in n.names))
    from_imports.sort(
        key=lambda n: (n.module or "")
        + ":"
        + ", ".join(a.asname or a.name for a in n.names)
    )
    tree.body = imports + from_imports + others
    return tree


def _annotate_simple_types(tree: ast.Module) -> ast.Module:
    # Varsayılanı literal olan ve annotasyonu olmayan parametreler için basit type hint
    for node in ast.walk(tree):
        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
            args = node.args
            defaults = list(args.defaults)
            nondefault_count = len(args.args) - len(defaults)
            for i, a in enumerate(args.args):
                if a.annotation is not None:
                    continue
                if i < nondefault_count:
                    continue
                d = defaults[i - nondefault_count]
                ann = None
                if isinstance(d, ast.Constant):
                    if isinstance(d.value, bool):
                        ann = ast.Name(id="bool", ctx=ast.Load())
                    elif isinstance(d.value, int):
                        ann = ast.Name(id="int", ctx=ast.Load())
                    elif isinstance(d.value, float):
                        ann = ast.Name(id="float", ctx=ast.Load())
                    elif isinstance(d.value, str):
                        ann = ast.Name(id="str", ctx=ast.Load())
                elif isinstance(d, ast.List):
                    ann = ast.Name(id="list", ctx=ast.Load())
                elif isinstance(d, ast.Dict):
                    ann = ast.Name(id="dict", ctx=ast.Load())
                elif isinstance(d, ast.Tuple):
                    ann = ast.Name(id="tuple", ctx=ast.Load())
                if ann is not None:
                    a.annotation = ann
    return tree


def _format_with_black_if_available(text: str) -> str:
    try:
        import black

        return black.format_str(text, mode=black.Mode())
    except Exception:
        return text


def _local_edit(src: str, instruction: str, language: str) -> str:
    """
    Local, model-free edit: ensure docstrings; remove unused imports; simple type hints; sort imports.
    """
    try:
        tree = ast.parse(src)
    except SyntaxError:
        return _strip_code_fences(src)

    tree = _ensure_docstrings(tree, language)
    tree = _remove_unused_imports(tree)
    tree = _annotate_simple_types(tree)
    tree = _sort_imports(tree)

    try:
        new_src = ast.unparse(tree)  # Python 3.9+
    except Exception:
        return _strip_code_fences(src)

    # Ensure from __future__ import annotations at top if missing
    lines = new_src.splitlines()
    if not any(l.strip() == "from __future__ import annotations" for l in lines[:5]):
        insert_at = 0
        for i, l in enumerate(lines[:10]):
            if (
                l.startswith("#!")
                or l.startswith("# -*-")
                or l.startswith("# Generated")
            ):
                insert_at = i + 1
        lines.insert(insert_at, "from __future__ import annotations")
        new_src = "\n".join(lines)

    return _format_with_black_if_available(new_src)


# ----------------------- Prompt templates -----------------------------

PROMPT_TEMPLATES = {
    "sys_editor": """You are a precise, deterministic Python CODE EDITOR.
- Apply the user's INSTRUCTION to a SINGLE FILE and return ONLY the full, final file content.
- Output MUST be wrapped between <FILE> and </FILE>. No extra text, no Markdown fences.
- Determinism: for the same input, produce the same output; do not inject arbitrary changes.
- Preserve: license header, encoding cookie, shebang, module-level dunders (__all__, __version__), public API, CLI behavior, and runtime semantics unless explicitly requested.
- Keep 'from __future__ import annotations' if present. Do not change line endings or file encoding markers.
- Imports: remove unused; avoid adding new third-party deps unless explicitly required; order groups as stdlib / third-party / local.
- Style: PEP 8; concise comments; no noisy changes.
- Docstrings:
  * MUST be proper triple-double-quoted string literals (\"\"\"...\") as the first statement of the module/function/class body.
  * Do NOT wrap a docstring literal inside another string (forbidden: '\"\"\"text\"\"\"').
  * If you cannot confidently document parameters/returns, OMIT such sections entirely instead of writing placeholders.
  * The words TODO / FIXME / TBD are strictly forbidden anywhere.
- If the instruction is vague, do a safe readability refactor (naming, tiny helpers) WITHOUT changing public names, signatures, I/O formats, or exception types.
- If no changes are needed, return the original file exactly.
{lang_directive}
""",
    "user_editor": """# METADATA
FILENAME: {filename}
LANGUAGE: python
PYTHON_VERSION: {py_version}

# INSTRUCTION
<INSTRUCTION>
{instruction}
</INSTRUCTION>

# ORIGINAL
<ORIGINAL>
{src}
</ORIGINAL>

# RESPONSE CONSTRAINTS
- Return ONLY the complete revised file content between <FILE> and </FILE>.
- No commentary, no diffs, no Markdown fences.
- Single-file scope: do not reference or create other files.

# RESPONSE FORMAT
<FILE>
...full revised file content...
</FILE>
""",
    "gen_editor": """You are a senior Python refactoring assistant.
- Output a single complete Python file (no Markdown, no commentary).
- Keep behavior unless the instruction says otherwise; preserve headers, public API, CLI behavior, dunders.
- Maintain 'from __future__ import annotations' if present; do not invent unused imports or new dependencies.
- Docstrings must be proper triple-double-quoted literals; do not write placeholders; OMIT unknown sections.
- Remove unused imports; prefer idiomatic, PEP 8 style.

{lang_directive}

### Instruction
{instruction}

### Original code
{src}

### Edited code (raw Python only)
""",
}


# --------------------------- AI editing core --------------------------


def _try_parse_python(text: str) -> bool:
    try:
        ast.parse(text)
        return True
    except Exception:
        return False


# --- yardımcı kontroller (bir kez ekleyin) ---
_FORBIDDEN_TOKENS_RE = re.compile(r"\b(?:TODO|TBD|FIXME)\b", re.IGNORECASE)
# ör. '"""doc"""' gibi tek tırnak içine gömülü üçlü çift tırnak artefaktlarını yakalar
_QUOTE_ARTIFACTS_RE = re.compile(
    r"(\'\s*\"\"\"|\"\"\"\s*\'|\'\'\'\s*\"\"\"|\"\"\"\s*\'\'\')"
)


def _has_forbidden_tokens(text: str) -> bool:
    return bool(_FORBIDDEN_TOKENS_RE.search(text or ""))


def _has_docstring_quote_artifacts(text: str) -> bool:
    return bool(_QUOTE_ARTIFACTS_RE.search(text or ""))


def _passes_strict_validations(text: str) -> bool:
    if not text or not text.strip():
        return False
    if _has_forbidden_tokens(text):
        return False
    if _has_docstring_quote_artifacts(text):
        return False
    try:
        ast.parse(text)
    except Exception:
        return False
    return True


def get_ai_edited(
    src: str,
    filename: str,
    instruction: str,
    language: str,
    base_url: str,
    model: str,
) -> str:
    lang_dir = _lang_directive(language)

    # 1) Chat tarzı dene (tercihli)
    sys_prompt = PROMPT_TEMPLATES["sys_editor"].format(lang_directive=(lang_dir or ""))
    user_prompt = PROMPT_TEMPLATES["user_editor"].format(
        filename=filename or "input.py",
        py_version=f"{sys.version_info.major}.{sys.version_info.minor}",
        instruction=instruction,
        src=src,
    )
    messages = [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": user_prompt},
    ]
    raw = ai_chat_ollama(messages, model=model, base_url=base_url) or ""
    m = re.search(r"<FILE>(.*)</FILE>", raw, flags=re.DOTALL | re.IGNORECASE)
    edited = _strip_code_fences((m.group(1) if m else raw).strip())

    # Sıkı doğrulamalar: yasaklı kelimeler, docstring artefaktı ve parse
    if _passes_strict_validations(edited) and not _quality_too_low(edited, src):
        return edited

    # 2) Fallback: /api/generate ile doğrudan üret
    gen_prompt = PROMPT_TEMPLATES["gen_editor"].format(
        instruction=instruction, src=src, lang_directive=(lang_dir or "")
    )
    edited2 = _strip_code_fences(_ollama_generate(gen_prompt, base_url, model) or "")

    if _passes_strict_validations(edited2) and not _quality_too_low(edited2, src):
        return edited2

    # 3) Yerel güvenli düzenleme (TODO/artefakt üretmez)
    return _local_edit(src, instruction, language)


@app.get("/license", include_in_schema=False)
def license_txt():
    with open(resource_path("LICENSE"), "r", encoding="utf-8") as f:
        return PlainTextResponse(f.read())


# ------------------------------ API'ler -------------------------------


@app.post("/plan")
async def plan(
    file: UploadFile = File(...),
    compact: int = Form(0),
    pack_small_lines: int = Form(40),
    max_modules: int = Form(12),
    min_module_lines: int = Form(0),
    target_modules: Optional[int] = Form(None),
    ai_name: bool = Form(True),
    ollama_base_url: str = Form(DEFAULT_OLLAMA_BASE_URL),
    ollama_model: str = Form(DEFAULT_OLLAMA_MODEL),
):
    try:
        src = (await file.read()).decode("utf-8", errors="replace")
        pack_small_lines, max_modules, min_module_lines, target_modules = apply_compact(
            compact, pack_small_lines, min_module_lines, max_modules, target_modules
        )
        plan_obj = analyze_source(
            src,
            pack_small_lines=pack_small_lines,
            max_modules=max_modules,
            min_module_lines=min_module_lines,
            target_modules=target_modules,
        )
        suggested = None
        if ai_name:
            tree = ast.parse(src)
            items, _, _, _ = extract_top_level_items(src, tree)
            suggested = ai_suggest_name(
                input_path=Path(file.filename or "input.py"),
                src=src,
                tree=tree,
                items=items,
                provider="ollama",
                model=ollama_model,
                base_url=ollama_base_url,
            )
        return JSONResponse({"plan": plan_obj, "ai_name": suggested})
    except SyntaxError as e:
        return JSONResponse({"error": f"SyntaxError: {e}"}, status_code=400)


@app.post("/plan_multi")
async def plan_multi(
    files: List[UploadFile] = File(...),
    compact: int = Form(0),
    pack_small_lines: int = Form(40),
    max_modules: int = Form(12),
    min_module_lines: int = Form(0),
    target_modules: Optional[int] = Form(None),
    ai_name: bool = Form(True),
    ollama_base_url: str = Form(DEFAULT_OLLAMA_BASE_URL),
    ollama_model: str = Form(DEFAULT_OLLAMA_MODEL),
):
    pack_small_lines, max_modules, min_module_lines, target_modules = apply_compact(
        compact, pack_small_lines, min_module_lines, max_modules, target_modules
    )
    results = []
    for f in files:
        try:
            src = (await f.read()).decode("utf-8", errors="replace")
            tree = ast.parse(src)
            p = analyze_source(
                src,
                pack_small_lines=pack_small_lines,
                max_modules=max_modules,
                min_module_lines=min_module_lines,
                target_modules=target_modules,
            )
            suggested = None
            if ai_name:
                items, _, _, _ = extract_top_level_items(src, tree)
                suggested = ai_suggest_name(
                    input_path=Path(f.filename or "input.py"),
                    src=src,
                    tree=tree,
                    items=items,
                    provider="ollama",
                    model=ollama_model,
                    base_url=ollama_base_url,
                )
            results.append({"filename": f.filename, "plan": p, "ai_name": suggested})
        except SyntaxError as e:
            results.append({"filename": f.filename, "error": f"SyntaxError: {e}"})
    return JSONResponse({"files": results})


@app.post("/build")
async def build(
    file: UploadFile = File(...),
    package_name: Optional[str] = Form(None),
    compact: int = Form(0),
    pack_small_lines: int = Form(40),
    max_modules: int = Form(12),
    min_module_lines: int = Form(0),
    target_modules: Optional[int] = Form(None),
    ai_name: bool = Form(True),
    ollama_base_url: str = Form(DEFAULT_OLLAMA_BASE_URL),
    ollama_model: str = Form(DEFAULT_OLLAMA_MODEL),
):
    src_bytes = await file.read()
    src = src_bytes.decode("utf-8", errors="replace")
    pack_small_lines, max_modules, min_module_lines, target_modules = apply_compact(
        compact, pack_small_lines, min_module_lines, max_modules, target_modules
    )
    workdir = Path(tempfile.mkdtemp())
    try:
        input_path = workdir / (file.filename or "input.py")
        input_path.write_text(src, encoding="utf-8")

        try:
            tree = ast.parse(src)
        except SyntaxError as e:
            return JSONResponse({"error": f"SyntaxError: {e}"}, status_code=400)

        items, imports, main_block, mod_doc = extract_top_level_items(src, tree)

        components, name_to_module = build_components(items)
        components, name_to_module = repack_components(
            base_file_stem=input_path.stem,
            components=components,
            name_to_module=name_to_module,
            group_assignments=True,
            pack_small_lines=pack_small_lines,
            max_modules=max_modules,
            min_module_lines=min_module_lines,
            target_modules=target_modules,
        )

        final_pkg = package_name
        if not final_pkg and ai_name:
            final_pkg = ai_suggest_name(
                input_path=input_path,
                src=src,
                tree=tree,
                items=items,
                provider="ollama",
                model=ollama_model,
                base_url=ollama_base_url,
            )
        if not final_pkg:
            final_pkg = (
                Path(file.filename or "pkg").stem.replace("-", "_") or "module"
            ).lower()

        out_dir = workdir / final_pkg
        out_dir.mkdir(parents=True, exist_ok=True)

        module_names = []
        for comp in components:
            mod_path = out_dir / f"{comp.module_name}.py"
            mod_text = render_module(comp, imports, name_to_module)
            mod_path.write_text(mod_text, encoding="utf-8")
            _postprocess_module_file(mod_path)
            module_names.append(comp.module_name)

        init_text = render_init(components, pkg_doc=mod_doc)
        (out_dir / "__init__.py").write_text(init_text, encoding="utf-8")

        has_main = False
        if main_block:
            body = extract_main_block_code(main_block)
            main_text = render_main(init_reexport=True, main_body=body)
            (out_dir / "__main__.py").write_text(main_text, encoding="utf-8")
            has_main = True

        _write_runner_single(final_pkg, out_dir, module_names, has_main)

        # ZIP oluştur
        zip_buf = io.BytesIO()
        with zipfile.ZipFile(zip_buf, "w", zipfile.ZIP_DEFLATED) as zf:
            for path in out_dir.rglob("*"):
                zf.write(path, arcname=str(Path(final_pkg) / path.relative_to(out_dir)))
        zip_buf.seek(0)
        headers = {"Content-Disposition": f"attachment; filename={final_pkg}.zip"}
        return StreamingResponse(zip_buf, media_type="application/zip", headers=headers)
    finally:
        shutil.rmtree(workdir, ignore_errors=True)


@app.post("/build_multi")
async def build_multi(
    files: List[UploadFile] = File(...),
    compact: int = Form(0),
    pack_small_lines: int = Form(40),
    max_modules: int = Form(12),
    min_module_lines: int = Form(0),
    target_modules: Optional[int] = Form(None),
    ai_name: bool = Form(True),
    ollama_base_url: str = Form(DEFAULT_OLLAMA_BASE_URL),
    ollama_model: str = Form(DEFAULT_OLLAMA_MODEL),
):
    pack_small_lines, max_modules, min_module_lines, target_modules = apply_compact(
        compact, pack_small_lines, min_module_lines, max_modules, target_modules
    )
    workdir = Path(tempfile.mkdtemp())
    root_out = workdir / "packages"
    root_out.mkdir(parents=True, exist_ok=True)

    pkg_infos: List[Dict[str, Any]] = []

    try:
        used_pkg_names = set()
        for f in files:
            src = (await f.read()).decode("utf-8", errors="replace")
            try:
                tree = ast.parse(src)
            except SyntaxError as e:
                # Hatalı dosyayı atla ve rapora ekle
                (
                    root_out / f"ERROR_{Path(f.filename or 'file.py').name}.txt"
                ).write_text(f"SyntaxError: {e}", encoding="utf-8")
                continue

            items, imports, main_block, mod_doc = extract_top_level_items(src, tree)
            components, name_to_module = build_components(items)
            components, name_to_module = repack_components(
                base_file_stem=(Path(f.filename or "input.py").stem),
                components=components,
                name_to_module=name_to_module,
                group_assignments=True,
                pack_small_lines=pack_small_lines,
                max_modules=max_modules,
                min_module_lines=min_module_lines,
                target_modules=target_modules,
            )
            pkg_name = None
            if ai_name:
                pkg_name = ai_suggest_name(
                    input_path=Path(f.filename or "input.py"),
                    src=src,
                    tree=tree,
                    items=items,
                    provider="ollama",
                    model=ollama_model,
                    base_url=ollama_base_url,
                )
            if not pkg_name:
                pkg_name = (
                    Path(f.filename or "pkg").stem.replace("-", "_") or "module"
                ).lower()

            base = pkg_name
            suff = 2
            while (root_out / pkg_name).exists() or pkg_name in used_pkg_names:
                pkg_name = f"{base}_{suff}"
                suff += 1
            used_pkg_names.add(pkg_name)

            out_dir = root_out / pkg_name
            out_dir.mkdir(parents=True, exist_ok=True)

            module_names = []
            for comp in components:
                mod_path = out_dir / f"{comp.module_name}.py"
                mod_text = render_module(comp, imports, name_to_module)
                mod_path.write_text(mod_text, encoding="utf-8")
                _postprocess_module_file(mod_path)
                module_names.append(comp.module_name)

            init_text = render_init(components, pkg_doc=mod_doc)
            (out_dir / "__init__.py").write_text(init_text, encoding="utf-8")

            has_main = False
            if main_block:
                body = extract_main_block_code(main_block)
                main_text = render_main(init_reexport=True, main_body=body)
                (out_dir / "__main__.py").write_text(main_text, encoding="utf-8")
                has_main = True

            _write_runner_single(pkg_name, out_dir, module_names, has_main)
            pkg_infos.append(
                {"name": pkg_name, "modules": module_names, "has_main": has_main}
            )

        _write_runner_multi(pkg_infos, root_out)

        zip_buf = io.BytesIO()
        with zipfile.ZipFile(zip_buf, "w", zipfile.ZIP_DEFLATED) as zf:
            for path in root_out.rglob("*"):
                zf.write(path, arcname=str(path.relative_to(root_out)))
        zip_buf.seek(0)
        headers = {"Content-Disposition": "attachment; filename=packages.zip"}
        return StreamingResponse(zip_buf, media_type="application/zip", headers=headers)
    finally:
        shutil.rmtree(workdir, ignore_errors=True)


# -------------------------- AI edit endpoints -------------------------


@app.post("/ai_edit")
async def ai_edit(
    file: UploadFile = File(...),
    instruction: str = Form(""),
    language: str = Form("auto"),
    ollama_base_url: str = Form(DEFAULT_OLLAMA_BASE_URL),
    ollama_model: str = Form(DEFAULT_OLLAMA_MODEL),
):
    src = (await file.read()).decode("utf-8", errors="replace")
    instruction = (
        instruction
        or "Refactor the code for clarity, add missing docstrings, complete type hints, and remove unused imports."
    )
    edited = get_ai_edited(
        src,
        file.filename or "input.py",
        instruction,
        language,
        ollama_base_url,
        ollama_model,
    )
    return JSONResponse({"edited": edited})


@app.post("/ai_edit_multi")
async def ai_edit_multi(
    files: List[UploadFile] = File(None),
    file: UploadFile = File(None),
    instruction: str = Form(""),
    language: str = Form("auto"),
    ollama_base_url: str = Form(DEFAULT_OLLAMA_BASE_URL),
    ollama_model: str = Form(DEFAULT_OLLAMA_MODEL),
):
    uploads: List[UploadFile] = []
    if files:
        uploads.extend(files)
    if file:
        uploads.append(file)
    if not uploads:
        return JSONResponse(
            {"error": "No files received. Send 'files' (list) or 'file'."},
            status_code=400,
        )

    instruction = (
        instruction
        or "Refactor the code for clarity, add missing docstrings, complete type hints, and remove unused imports."
    )
    workdir = Path(tempfile.mkdtemp())
    try:
        out_dir = workdir / "edited"
        out_dir.mkdir(parents=True, exist_ok=True)
        for idx, f in enumerate(uploads):
            src = (await f.read()).decode("utf-8", errors="replace")
            edited = get_ai_edited(
                src,
                f.filename or f"file_{idx}.py",
                instruction,
                language,
                ollama_base_url,
                ollama_model,
            )
            safe = Path(f.filename or f"file_{idx}.py").name
            (out_dir / f"{idx:03d}_{safe}").write_text(edited, encoding="utf-8")
        zip_buf = io.BytesIO()
        with zipfile.ZipFile(zip_buf, "w", zipfile.ZIP_DEFLATED) as zf:
            for p in out_dir.rglob("*"):
                zf.write(p, arcname=str(p.relative_to(out_dir)))
        zip_buf.seek(0)
        headers = {"Content-Disposition": "attachment; filename=edited_files.zip"}
        return StreamingResponse(zip_buf, media_type="application/zip", headers=headers)
    finally:
        shutil.rmtree(workdir, ignore_errors=True)
